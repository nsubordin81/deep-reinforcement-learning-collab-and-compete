{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tennis Task for Collaboration and Competition\r\n",
    "---\r\n",
    "\r\n",
    "### 1. Import the Necessary Packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from unityagents import UnityEnvironment\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "import numpy as np\r\n",
    "from collections import deque\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from utilities import transpose_list, transpose_to_tensor\r\n",
    "from buffer import ReplayBuffer\r\n",
    "\r\n",
    "from maddpg_agent import MADDPG\r\n",
    "\r\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Instantiate the Environment and Agent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "env = UnityEnvironment(file_name=\"./Tennis_Windows_x86_64/Tennis.exe\")\r\n",
    "brain_name = env.brain_names[0]\r\n",
    "brain = env.brains[brain_name]\r\n",
    "START_DECAY = 10000\r\n",
    "GOAL_SCORE = .5"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Train the Agent with DDPG"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "BUFFER_SIZE = int(5e6)  # replay buffer size\r\n",
    "BATCH_SIZE = 128  # minibatch size\r\n",
    "GAMMA = 0.99  # discount factor\r\n",
    "TAU = 1e-3  # for soft update of target parameters\r\n",
    "LR_ACTOR = 1e-4  # learning rate of the actor\r\n",
    "LR_CRITIC = 1e-4  # learning rate of the critic\r\n",
    "WEIGHT_DECAY = 0 # L2 penalty to the cost function to shrink weights\r\n",
    "INITIAL_NOISE = 2 # how much OU Noise to start with\r\n",
    "NOISE_DECAY = 0.9999 # how fast to decay OU noise\r\n",
    "UPDATE_EVERY = 4 # how often to update the \r\n",
    "SOLVED = False # whether the task has been solved or not\r\n",
    "\r\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "\r\n",
    "\r\n",
    "def main(number_of_episodes=500, num_agents = 2, print_every=100):\r\n",
    "\r\n",
    "    scores_deque = deque(maxlen=print_every)\r\n",
    "    scores = []\r\n",
    "    torch.set_num_threads = num_agents\r\n",
    "    shared_buffer = ReplayBuffer(BUFFER_SIZE)\r\n",
    "    maddpg = MADDPG()\r\n",
    "    agent1_reward = []\r\n",
    "    agent2_reward = []\r\n",
    "\r\n",
    "    # training loop\r\n",
    "    # show progressbar\r\n",
    "    import progressbar as pb\r\n",
    "    widget = ['episode: ', pb.Counter(),'/',str(number_of_episodes),' ', \r\n",
    "              pb.Percentage(), ' ', pb.ETA(), ' ', pb.Bar(marker=pb.RotatingMarker()), ' ' ]\r\n",
    "    timer    = pb.ProgressBar(widgets=widget, maxval=number_of_episodes).start()\r\n",
    "    noise = INITIAL_NOISE\r\n",
    "\r\n",
    "\r\n",
    "    for i_episode in range(1, number_of_episodes+1):\r\n",
    "        timer.update(i_episode)\r\n",
    "        reward_this_episode = np.zeros((num_agents, 2)) # TODO why is this number 2?\r\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\r\n",
    "        all_observations = env_info.vector_observations\r\n",
    "        import pdb; pdb.set_trace()\r\n",
    "        obs, obs_full = transpose_list(all_observations)\r\n",
    "\r\n",
    "        agent.reset()\r\n",
    "        score = 0\r\n",
    "        t = 0\r\n",
    "\r\n",
    "        while True:\r\n",
    "            t += num_agents\r\n",
    "            actions = maddpg.act(transpose_to_tensor(obs), noise=noise)\r\n",
    "            noise *= NOISE_DECAY\r\n",
    "\r\n",
    "            actions_array = torch.stack(actions).detach(numpy())\r\n",
    "\r\n",
    "            actions_for_env = np.rollaxis(actions_array, 1)\r\n",
    "\r\n",
    "            next_obs, next_obs_full, rewards, dones, info = env.step(actions_for_env)[brain_name]\r\n",
    "\r\n",
    "            transition = (obs, obs_full, actions_for_env, rewards, next_obs, next_obs_full, dones)\r\n",
    "\r\n",
    "            shared_buffer.push(transition)\r\n",
    "\r\n",
    "            reward_this_episode += rewards\r\n",
    "\r\n",
    "            obs, obs_full = next_obs, next_obs_full\r\n",
    "\r\n",
    "        if len(buffer) > BATCH_SIZE and episode % episode_per_update < parallel_envs:\r\n",
    "            for a_i in range(2):\r\n",
    "                samples = buffer.sample(batchsize)\r\n",
    "                maddpg.update(samples, a_i, logger)\r\n",
    "            maddpg.update_targets()\r\n",
    "\r\n",
    "        for i in range(num_agents):\r\n",
    "            agent0_reward.append(reward_this_episode[i, 0]) \r\n",
    "            agent1_reward.append(reward_this_episode[i, 1]) \r\n",
    "        \r\n",
    "        if episode % 100 == 0 or episode == number_of_episodes-1:\r\n",
    "            avg_rewards = [np.mean(agent0_reward), np.mean(agent1_reward)]\r\n",
    "            agent0_reward = []\r\n",
    "            agent1_reward = []\r\n",
    "            for a_i, avg_reward in enumerate(avg_rewards):\r\n",
    "                logger.add_scalar('agent%i/mean_episode_rewards' % a_i, avg_reward, episode)\r\n",
    "\r\n",
    "\r\n",
    "            # actions = [agent.act(state) for state in states]\r\n",
    "            # print(f\"actions {actions}\")\r\n",
    "            # actions = np.vstack(actions)\r\n",
    "            # env_info = env.step(actions)[brain_name]\r\n",
    "            # next_states = env_info.vector_observations\r\n",
    "            # rewards = env_info.rewards\r\n",
    "            # dones = env_info.local_done\r\n",
    "            # agent.step(states, actions, rewards, next_states, dones)\r\n",
    "            # states = next_states\r\n",
    "            # score += np.amax(rewards)\r\n",
    "            # assumption, episode will terminate for both agents at the same time\r\n",
    "            if True in dones:\r\n",
    "                break \r\n",
    "        scores_deque.append(avg_rewards)\r\n",
    "        scores.append(avg_rewards)\r\n",
    "        print('\\rEpisode {}\\tAverage Scores: {:.2f}'.format(i_episode, np.mean(scores_deque)), end=\"\")\r\n",
    "        if np.amax(np.mean(scores_deque)) >= GOAL_SCORE and not SOLVED:\r\n",
    "            SOLVED = True\r\n",
    "            print(f'Environment solved! achieved an average score of 30 over 100 episodes at episode {i_episode}')\r\n",
    "            #saving model\r\n",
    "            save_dict_list =[]\r\n",
    "            if save_info:\r\n",
    "                for i in range(3):\r\n",
    "\r\n",
    "                    save_dict = {'actor_params' : maddpg.maddpg_agent[i].actor.state_dict(),\r\n",
    "                                'actor_optim_params': maddpg.maddpg_agent[i].actor_optimizer.state_dict(),\r\n",
    "                                'critic_params' : maddpg.maddpg_agent[i].critic.state_dict(),\r\n",
    "                                'critic_optim_params' : maddpg.maddpg_agent[i].critic_optimizer.state_dict()}\r\n",
    "                    save_dict_list.append(save_dict)\r\n",
    "\r\n",
    "                    torch.save(save_dict_list, \r\n",
    "                            os.path.join(model_dir, 'episode-{}.pt'.format(episode)))\r\n",
    "        if i_episode % print_every == 0:\r\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\r\n",
    "                        \r\n",
    "    return scores\r\n",
    "\r\n",
    "scores = main()\r\n",
    "\r\n",
    "fig = plt.figure()\r\n",
    "ax = fig.add_subplot(111)\r\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\r\n",
    "plt.ylabel('Score')\r\n",
    "plt.xlabel('Episode #')\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "episode: 0/500 N/A% ETA:  --:--:-- |/                                        | "
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-cc5087fb8969>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-cc5087fb8969>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(number_of_episodes, num_agents, print_every)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0menv_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mall_observations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtranspose_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_observations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Watch a Smart Agent!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def run_agent(n_episodes=1, max_t=300, print_every=100):\r\n",
    "    score = 0\r\n",
    "    agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\r\n",
    "    agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\r\n",
    "\r\n",
    "    for i_episode in range(1, n_episodes+1):\r\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\r\n",
    "        state = env_info.vector_observations\r\n",
    "        agent.reset()\r\n",
    "\r\n",
    "        while True:\r\n",
    "            action = agent.act(state, add_noise=False, decay_noise=decay)\r\n",
    "            env_info = env.step(action)[brain_name]\r\n",
    "            next_state = env_info.vector_observations\r\n",
    "            reward = env_info.rewards[0]\r\n",
    "            done = env_info.local_done[0]\r\n",
    "            state = next_state\r\n",
    "            score += reward\r\n",
    "            if done:\r\n",
    "                break \r\n",
    "    return score\r\n",
    "\r\n",
    "score = run_agent()\r\n",
    "\r\n",
    "print(f\"Agent Complete Episode With a Score Of: {score}\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f3fb8586f7c3502cc6081ea0fdb106bb87fbdd47e0345f6dae1027d9f2fdb36"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}